{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LossRouting:\n",
    "    loss_func: Callable\n",
    "    pred_idx: int\n",
    "    target_idx: int\n",
    "    weight: float = 1.0\n",
    "\n",
    "class CombinedLoss:\n",
    "    \"\"\"Applies loss functions to multiple model outputs and sums them. \n",
    "    If applicable, it can decode and compute activations for each model output.\"\"\"\n",
    "\n",
    "    def __init__(self, *loss_routings):\n",
    "        self.loss_routings = loss_routings\n",
    "    \n",
    "    def __call__(self, outs, *targets, **kwargs):\n",
    "        return sum([\n",
    "            routing.weight*routing.loss_func(outs[routing.pred_idx], targets[routing.target_idx]) \n",
    "            for routing in self.loss_routings\n",
    "        ])\n",
    "    \n",
    "    def activation(self, outs): \n",
    "        return [getattr(routing.loss_func, 'activation', noop)(outs[routing.pred_idx]) for routing in self.loss_routings]\n",
    "    \n",
    "    def decodes(self, outs):\n",
    "        return [getattr(routing.loss_func, 'decodes', noop)(outs[routing.pred_idx]) for routing in self.loss_routings]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_one_to_one_routing(cls, *loss_funcs):\n",
    "        return cls(*[LossRouting(loss_func, pred_idx=i, target_idx=i, weight=1.0) for i, loss_func in enumerate(loss_funcs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that a multi-task learning model produces two outputs:\n",
    "1. The logits for multi-class single-label classification, for which we want to use cross-entropy loss and softmax activation\n",
    "2. A logit for single-class classification, for which we want to use binary cross-entropy and sigmoid activation\n",
    "\n",
    "`CombinedLoss` enables using the corresponding loss function and its activation function for each model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "ce = CrossEntropyLossFlat()\n",
    "bce = BCEWithLogitsLossFlat()\n",
    "comb_loss = CombinedLoss.from_one_to_one_routing(ce, bce)\n",
    "\n",
    "bs = 8\n",
    "target1, output1 = torch.randint(0, 5, (bs,)), torch.randn(bs, 5) # 5 classes\n",
    "target2, output2 = torch.randint(0, 2, (bs,), dtype=float), torch.randn(bs)*10\n",
    "actual = comb_loss((output1, output2), target1, target2)\n",
    "\n",
    "loss1 = ce(output1, target1)\n",
    "loss2 = bce(output2, target2)\n",
    "expected = loss1 + loss2\n",
    "test_close(expected, actual)\n",
    "\n",
    "# activations\n",
    "actual_acts_output1, actual_acts_output2 = comb_loss.activation([output1, output2])\n",
    "expected_acts_output1, expected_acts_output2 = ce.activation(output1), bce.activation(output2)\n",
    "test_close(expected_acts_output1, actual_acts_output1)\n",
    "test_eq(expected_acts_output2, actual_acts_output2)\n",
    "\n",
    "# decoding\n",
    "actual_decoded_output1, actual_decoded_output2 = comb_loss.decodes([output1, output2])\n",
    "expected_decoded_output1, expected_decoded_output2 = ce.decodes(output1), bce.decodes(output2)\n",
    "test_close(expected_decoded_output1, actual_decoded_output1)\n",
    "test_eq(expected_decoded_output2, actual_decoded_output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are raw model outputs (logits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1850, -1.1469,  1.2270,  0.8119, -1.4351],\n",
       "         [-1.0543,  1.5350, -1.0777,  0.4074,  0.2599],\n",
       "         [ 0.1193,  2.9036,  0.1536,  1.0384, -1.2029],\n",
       "         [-0.8484,  1.8163,  0.6120,  1.1216,  0.4150],\n",
       "         [ 1.3586, -1.7410,  1.6133, -1.1984,  0.9948],\n",
       "         [ 0.8181, -1.2083, -1.2216, -0.7513,  0.2764],\n",
       "         [ 1.2261, -1.1696, -1.1735,  1.0326, -0.0525],\n",
       "         [ 0.2331,  0.1555,  1.1627, -1.2526, -0.0451]]),\n",
       " tensor([  3.2092,  -6.4007,  -0.3515,  -3.4715,  -1.5865, -16.1386,   9.8419,\n",
       "         -13.8744])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[output1, output2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applicable, it can decode the raw model outputs and compute activations. For instance, let's decode logits to class label indices and binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2, 1, 1, 1, 2, 0, 0, 2]),\n",
       " tensor([ True, False, False, False, False, False,  True, False])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_loss.decodes([output1, output2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, here are the activations for each model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1621, 0.0428, 0.4596, 0.3034, 0.0321],\n",
       "         [0.0429, 0.5709, 0.0419, 0.1849, 0.1595],\n",
       "         [0.0476, 0.7710, 0.0493, 0.1194, 0.0127],\n",
       "         [0.0329, 0.4728, 0.1418, 0.2360, 0.1164],\n",
       "         [0.3218, 0.0145, 0.4151, 0.0250, 0.2236],\n",
       "         [0.4874, 0.0642, 0.0634, 0.1015, 0.2835],\n",
       "         [0.4378, 0.0399, 0.0397, 0.3607, 0.1219],\n",
       "         [0.1837, 0.1700, 0.4655, 0.0416, 0.1391]]),\n",
       " tensor([9.6118e-01, 1.6577e-03, 4.1302e-01, 3.0135e-02, 1.6988e-01, 9.7967e-08,\n",
       "         9.9995e-01, 9.4281e-07])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_loss.activation([output1, output2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastmtl",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
