{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.basics import GetAttr, store_attr\n",
    "from types import FunctionType\n",
    "from fastai.metrics import Metric, AvgMetric, AccumMetric\n",
    "\n",
    "\n",
    "class _LearnerProxy(GetAttr):\n",
    "    _default = 'learn'\n",
    "    def __init__(self, learn, idx):\n",
    "        store_attr()\n",
    "        self.pred = self.learn.pred[idx]\n",
    "        self.y = self.learn.y[idx]\n",
    "\n",
    "\n",
    "class RoutedAccumMetric(AccumMetric, GetAttr):\n",
    "    \"AccumMetric with predictions and targets for a specific model head.\"\n",
    "    _default = 'metric'\n",
    "    def __init__(self, idx, metric):\n",
    "        self.idx = idx\n",
    "        self.metric = metric\n",
    "        self._name = metric.name\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Clear all targs and preds\"\n",
    "        return self.metric.reset()\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "        return self.metric.accumulate(_LearnerProxy(learn, self.idx))\n",
    "    \n",
    "    def __call__(self, preds, targs):\n",
    "        \"Calculate metric on one batch of data\"\n",
    "        return self.metric(preds[self.idx], targs[self.idx])\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self._name = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def route_to_metric(idx, metric):\n",
    "    \"\"\"Routes model output at idx to metric\"\"\"\n",
    "    if isinstance(metric, type): \n",
    "        metric = metric()\n",
    "    if isinstance(metric, FunctionType):\n",
    "        func = lambda preds, *targs, **kwargs: metric(preds[idx], targs[idx], **kwargs)\n",
    "        func.__name__ = metric.__name__\n",
    "        return AvgMetric(func)\n",
    "    if isinstance(metric, Metric):\n",
    "        return RoutedAccumMetric(idx, metric)\n",
    "    raise ValueError(\"Unsupported metric type; must be either function or Metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "import torch\n",
    "from fastai.metrics import accuracy, R2Score, F1Score\n",
    "\n",
    "bs = 8\n",
    "target1, pred1 = torch.randint(0, 5, [bs], dtype=float), torch.randint(0, 5, [bs], dtype=float)\n",
    "target2, pred2 = torch.randn(bs), torch.randn(bs)\n",
    "\n",
    "preds = [pred1, pred2]\n",
    "targets = [target1, target2]\n",
    "\n",
    "inner_f1_macro = F1Score(average='macro')\n",
    "f1_macro = route_to_metric(0, inner_f1_macro)\n",
    "test_close(f1_macro(preds, targets), inner_f1_macro(preds[0], targets[0]))\n",
    "\n",
    "inner_r2 = R2Score()\n",
    "r2 = route_to_metric(1, inner_r2)\n",
    "test_close(r2(preds, targets), inner_r2(preds[1], targets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def mtl_metrics(*metrics_list):\n",
    "    \"\"\"Convenience function to route each prediction to list of metrics by their order.\"\"\"\n",
    "    return [route_to_metric(i, m) for i, metrics in enumerate(metrics_list) for m in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "routed_metrics = mtl_metrics([F1Score(average='macro'), accuracy], [R2Score])\n",
    "test_eq(3, len(routed_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage for routing the first model prediction to `F1Score` and `accuracy`, while routing the second one to `R2Score`.\n",
    "```py\n",
    "learn = Learner(...\n",
    "    metrics = mtl_metrics([F1Score(average='macro'), accuracy], [R2Score])\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fastmtl')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
