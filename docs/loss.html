---

title: Loss


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/01_loss.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01_loss.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LossRouting" class="doc_header"><code>class</code> <code>LossRouting</code><a href="https://github.com/bdsaglam/fastmtl/tree/master/fastmtl/loss.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LossRouting</code>(<strong><code>loss_func</code></strong>:<code>typing.Callable</code>, <strong><code>pred_idx</code></strong>:<code>int</code>, <strong><code>target_idx</code></strong>:<code>int</code>, <strong><code>weight</code></strong>:<code>float</code>=<em><code>1.0</code></em>)</p>
</blockquote>
<p>LossRouting(loss_func: Callable, pred_idx: int, target_idx: int, weight: float = 1.0)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CombinedLoss" class="doc_header"><code>class</code> <code>CombinedLoss</code><a href="https://github.com/bdsaglam/fastmtl/tree/master/fastmtl/loss.py#L20" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CombinedLoss</code>(<strong>*<code>loss_routings</code></strong>)</p>
</blockquote>
<p>Applies loss functions to multiple model outputs and sums them.
If applicable, it can decode and compute activations for each model output.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Assume that a multi-task learning model produces two outputs:</p>
<ol>
<li>The logits for multi-class single-label classification, for which we want to use cross-entropy loss and softmax activation</li>
<li>A logit for single-class classification, for which we want to use binary cross-entropy and sigmoid activation</li>
</ol>
<p><a href="/fastmtl/loss.html#CombinedLoss"><code>CombinedLoss</code></a> enables using the corresponding loss function and its activation function for each model output.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">ce</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">()</span>
<span class="n">bce</span> <span class="o">=</span> <span class="n">BCEWithLogitsLossFlat</span><span class="p">()</span>
<span class="n">comb_loss</span> <span class="o">=</span> <span class="n">CombinedLoss</span><span class="o">.</span><span class="n">from_one_to_one_routing</span><span class="p">(</span><span class="n">ce</span><span class="p">,</span> <span class="n">bce</span><span class="p">)</span>

<span class="n">bs</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">target1</span><span class="p">,</span> <span class="n">output1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 5 classes</span>
<span class="n">target2</span><span class="p">,</span> <span class="n">output2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">comb_loss</span><span class="p">((</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">),</span> <span class="n">target1</span><span class="p">,</span> <span class="n">target2</span><span class="p">)</span>

<span class="n">loss1</span> <span class="o">=</span> <span class="n">ce</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">target1</span><span class="p">)</span>
<span class="n">loss2</span> <span class="o">=</span> <span class="n">bce</span><span class="p">(</span><span class="n">output2</span><span class="p">,</span> <span class="n">target2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">loss1</span> <span class="o">+</span> <span class="n">loss2</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span>

<span class="c1"># activations</span>
<span class="n">actual_acts_output1</span><span class="p">,</span> <span class="n">actual_acts_output2</span> <span class="o">=</span> <span class="n">comb_loss</span><span class="o">.</span><span class="n">activation</span><span class="p">([</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">])</span>
<span class="n">expected_acts_output1</span><span class="p">,</span> <span class="n">expected_acts_output2</span> <span class="o">=</span> <span class="n">ce</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output1</span><span class="p">),</span> <span class="n">bce</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output2</span><span class="p">)</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">expected_acts_output1</span><span class="p">,</span> <span class="n">actual_acts_output1</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">expected_acts_output2</span><span class="p">,</span> <span class="n">actual_acts_output2</span><span class="p">)</span>

<span class="c1"># decoding</span>
<span class="n">actual_decoded_output1</span><span class="p">,</span> <span class="n">actual_decoded_output2</span> <span class="o">=</span> <span class="n">comb_loss</span><span class="o">.</span><span class="n">decodes</span><span class="p">([</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">])</span>
<span class="n">expected_decoded_output1</span><span class="p">,</span> <span class="n">expected_decoded_output2</span> <span class="o">=</span> <span class="n">ce</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">output1</span><span class="p">),</span> <span class="n">bce</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">output2</span><span class="p">)</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">expected_decoded_output1</span><span class="p">,</span> <span class="n">actual_decoded_output1</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">expected_decoded_output2</span><span class="p">,</span> <span class="n">actual_decoded_output2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are raw model outputs (logits):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor([[ 9.6559e-01,  1.2909e+00, -2.0418e-01, -9.8991e-02,  5.9807e-01],
         [-6.1768e-01, -8.8121e-01, -9.5900e-03, -1.4741e+00, -5.2530e-01],
         [ 9.5259e-01,  1.2350e+00, -5.7586e-01, -6.4723e-02, -8.5460e-01],
         [ 1.3948e+00,  6.7017e-01,  2.4812e+00, -2.3243e+00,  4.6702e-01],
         [ 3.4889e-02, -2.5438e-01, -1.0769e+00, -9.6301e-02,  1.1432e+00],
         [-9.2353e-01, -4.6509e-01,  1.2955e+00,  3.1447e-01, -2.5700e+00],
         [ 8.2171e-01, -2.3441e-01, -4.7117e-01,  5.1372e-01,  7.5967e-01],
         [-5.2264e-01,  3.5434e-01,  2.9362e-01,  8.5736e-04, -1.8668e-01]]),
 tensor([-2.1232, 11.6096, -5.7914, -7.5502,  7.1219, -5.3170,  4.3356, 13.7366])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When applicable, it can decode the raw model outputs and compute activations. For instance, let's decode logits to class label indices and binary classes.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">comb_loss</span><span class="o">.</span><span class="n">decodes</span><span class="p">([</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor([1, 2, 1, 2, 4, 2, 0, 1]),
 tensor([False,  True, False, False,  True, False,  True,  True])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similary, here are the activations for each model output.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">comb_loss</span><span class="o">.</span><span class="n">activation</span><span class="p">([</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor([[0.2679, 0.3710, 0.0832, 0.0924, 0.1855],
         [0.1951, 0.1499, 0.3583, 0.0828, 0.2139],
         [0.3259, 0.4322, 0.0707, 0.1178, 0.0535],
         [0.2054, 0.0995, 0.6088, 0.0050, 0.0812],
         [0.1671, 0.1251, 0.0550, 0.1466, 0.5062],
         [0.0648, 0.1026, 0.5965, 0.2236, 0.0125],
         [0.3033, 0.1055, 0.0832, 0.2229, 0.2851],
         [0.1142, 0.2746, 0.2584, 0.1928, 0.1599]]),
 tensor([1.0686e-01, 9.9999e-01, 3.0443e-03, 5.2573e-04, 9.9919e-01, 4.8834e-03,
         9.8707e-01, 1.0000e+00])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

